# !/usr/bin/env python

import sys
#sys.path.insert(1, "/home/ross/EuPathGalaxy/Tools/lib/python/")
sys.path.insert(0, "/opt/galaxy/tools/eupath/Tools/lib/python")
from eupath import NewEupathExporter, ReferenceGenome
import optparse
import sys
import re
import os
from subprocess import check_output, call

"""
python EuPathGalaxy/Tools/bin/exportSNPDataToEuPathDB TestSetDataName DataSetSummary DataSetDescription rmadden.433744323@eupathdb.org /home/ross/EuPathGalaxy/Tools/lib/python exportToEuPathDBInfo.html /scratch/galaxy/files/016/dataset_16366.dat  SampleFileName

python EuPathGalaxy/Tools/bin/NewexportSNPDataToEuPathDB TestSetDataName DataSetSummary DataSetDescription rmadden.433744323@eupathdb.org /home/ross/EuPathGalaxy/Tools/lib/python exportToEuPathDBInfo.html /scratch/galaxy/files/016/dataset_16366.dat  SampleFileName TriTrypDB-29_TbruceiLister427_Genome



args = "$dataset_name" 
    "$summary" 
    "$description"
    "$__user_email__" 
    "$__tool_directory__" 
    "$output" 
"""


def main():

    parser = optparse.OptionParser()
    (options, args) = parser.parse_args()   

    # for i in args:
    #     print >> sys.stdout, i 
    # print >> sys.stdout, file_name    

    # So users are greeted with a bewildering traceback
    sys.tracebacklimit = 0

    # This class/function can be made in a new file where the file handler below can be created. 
    # maybe in Tools/bin/exportSNPDataToEuPathDB
    class FileValidator():

        def __init__(self):
            self.temp = ''

        def validate(self, file_to_val):

            print >> sys.stdout, "File to validate: ", file_to_val
            print >> sys.stdout, "---At validate. Beep boop.---"

            with open(file_to_val, mode='r') as file:

                bad_lines = []

                for line in file: 
                    #while i < 1000:
                    #line = next(file)                    
                    #print unicode(line[0:100], 'utf-8')
                    #if re.match(r"^.*\t\d*\t.*\t[ATGC]*\t[ATGC]*\t(\d*\.\d*|\d*)\t(\w*|\.)\t.*$", line):
                    if re.match(r"^.*\t\d*\t.*\t((([ATGCN]*),){1,}([ATGCN]*)|([ATGCN]*))\t((([ATGCN]*),){1,}([ATGCN]*)|([ATGCN]*))\t((\d*.\d*e-\d*)|(\d*\.\d*)|(\d*))\t(\w*|\.)\t.*$", line):
                        # This is attempting to match a line in the file where the data is stored. It assumes that the data is
                        # in the same order as above. Looking for string, tab, number, tab, sting, tab, ATC or G, tab ATC or G, tab
                        # int/float, tab
                        #print "-----", line
                        #print "Line ", i, " ok." 
                        pass
                    
                    elif unicode(line[0:2], 'utf-8') == '##':
                        # This is to ignore the headers that are added to the file, Jbrowse only cares about certain col headers
                        # and the data. VCFTools vcf-validator was too rigid to work here, kept failing due to the header being
                        # incorrect (even though the data looked like it was formatted correctly.)
                        pass

                    elif re.match(r"^#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\t.*$", line):
                        #elif '#CHROM' in line:
                        # This is testing for the header of the data. Note the wildcard at the end that will match any additions 
                        # by other programs that are not using the usual cols. 
                        # The order should not be an issue as it seems to be rigid.         
                        #print "-----", line
                        pass
                        
                    else:
                        bad_lines.append(line)

                if len(bad_lines) > 0:    
                    print >> sys.stderr,  "###### Error: Can't validate the file. ######"
                    print >> sys.stderr, "Hint: Is the data header formmatted: #CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT ?????"
                    print >> sys.stderr,  "###### These lines are not in the correct format for Jbrowse to read the data. ######"
                    for bad in bad_lines:            
                        print >> sys.stderr,  bad
                    raise ValueError(bad_lines)

                    # Redundancy.
                    sys.exit(1)

                elif len(bad_lines) == 0:
                    print >> sys.stderr, "File looks ok."                  
                    
                # raise ValueError('All broken!')


    class VCFFileHandler(NewEupathExporter.BaseFileHandler):
        """
        A class for handling the export of VCF file.
        The FileCollector and the Exporter can be overwritten if needed. 
        The validator should be bespoke to a file type - this runs on intilization for this class.
        """

        # Constants
        TYPE = "VCFFile"
        VERSION = "1.0"        

        def __init__(self, validator):

            self.parse_params(args)
            print >> sys.stderr, 'Other params:', self._other_params

	    print >> sys.stderr, "Genome", self._other_params[2]
	                
	    NewEupathExporter.BaseFileHandler.__init__(self, 
            VCFFileHandler.TYPE, 
            VCFFileHandler.VERSION,
            NewEupathExporter.FileCollector(self._other_params[0], self._other_params[1]),
            # TODO - better output sucess page handling.
            NewEupathExporter.Exporter(self._tool_directory, self._output, self.TYPE, self.VERSION, self._user_id, self._dataset_name, self._summary, self._description, self._other_params[2]),
            validator,
            args
            )
        
            try:
                self.validation(self._other_params[0])
            except:
                raise Exception("Validation failed.")
		

        def test(self):
            # Works
            print >> sys.stdout, "_tool_directory:", self._exporter._tool_directory
            # print >> sys.stdout, "_tool_directory:", self._exporter.collect_rest_data()
            print >> sys.stdout, "_dataset_name:", self._dataset_name
	        # print >> sys.stderr, "ID dep:", self.identify_dependencies()
	        # print >> sys.stderr, "ID project", self.identify_projects()
            # Testing
            self.identify_dataset_files()
            print >> sys.stderr, self._filecollector._datasetInfos

            # print >> sys.stdout, "_datasetInfos:", self._filecollector._datasetInfos  
             
        def validation(self, file):
            print >> sys.stdout, "--- Try validation. ---"
            self._validator.validate(file)
            print >> sys.stdout, "--- Validation success. ---" 

        def index_and_zip(self):
            """
            If the validation is succesful this overwites the _datasetInfos with the path of the zipped and indexed file.
            This is then used for the export and the original file is left for Galaxy uses.
            """
            print >> sys.stdout, "--- Zip and index .---", self._other_params[0]
           
            # Getting only the file name.
            rev_file = str(self._other_params[0])
            rev_file = rev_file.split('/')[-1]

            print >> sys.stderr, 'Rev_file', rev_file

            # # original file is still in place. 

            tmp_file = '/tmp/' + self._user_id + '/' +  rev_file
            print >> sys.stderr, tmp_file

            # Copy to /tmp/galaxy. Zip and index. 
            # Have a dir with unique user ID. 
            call(['mkdir', '/tmp/' + self._user_id])
            # Must be a better way to reference samtools - tabix and bgzip. - Works as is though.
            print >> sys.stderr, 'Copy to /tmp'
            call(['cp', self._other_params[0], tmp_file])

            # Rename file. 
            renamed_file = '/tmp/' + self._user_id + '/' + self._other_params[1] + '.vcf'
            call(['mv', tmp_file, renamed_file])
            print >> sys.stderr, 'Zipping....'
            call(['/mnt/galaxyTools/tools/tabix/0.2.6/bgzip', '-f', renamed_file])
            print >> sys.stderr, 'Indexing....'
            
            try:
                call(['/mnt/galaxyTools/tools/tabix/0.2.6/tabix', '-p', 'vcf', renamed_file + '.gz']) 
            except:
                call(['/mnt/galaxyTools/tools/tabix/0.2.6/tabix', '-pf', 'vcf', renamed_file + '.gz']) 

            print >> sys.stderr, 'Indexed'
            
            # Updating the _datasetInfos data.
            # New file path is updated in the _datasetInfos attribute.
            zipped_file_to_export = renamed_file + '.gz'
            index_file_to_export = renamed_file + '.gz' + '.tbi' 
            # print >> sys.stderr, self._filecollector._datasetInfos
            self._filecollector._datasetInfos[0]['path'] = zipped_file_to_export

            vcf_data = {'path': zipped_file_to_export , 'name': self._other_params[1]}
            index_data = {'path': index_file_to_export, 'name': self._other_params[1] + "_index"}

            self._filecollector._datasetInfos = [vcf_data, index_data]


            # open manifest file
            manifestPath = "/tmp/manifest." + str(os.getpid()) + ".txt"
            # print >> sys.stderr, 'Manifest:',  manifestPath
            manifest = open(manifestPath, "w+")

            ## Note in the xml this will need the right variables passed in - e.g. sample name, file format.
            print >> manifest, zipped_file_to_export  + "\t" + self._other_params[1]
            print >> manifest, index_file_to_export + "\t" + self._other_params[1] + "_index"               
            self._filecollector._datasetInfos.append({"name": "manifest.txt", "path": manifestPath})
            # print >> sys.stderr, "DATASETINFOS", self._filecollector._datasetInfos 
            manifest.close()




            # print >> sys.stderr, self._filecollector._datasetInfos[0]['path']
            # self._filecollector._datasetInfos.insert(1, self._filecollector._datasetInfos[0])
            # print >> sys.stderr, 'EXPORT DATA', self._filecollector._datasetInfos
            # self._filecollector._datasetInfos[1]['path'] = index_file_to_export
            # print >> sys.stderr, 'EXPORT DATA', self._filecollector._datasetInfos

            return self._filecollector._datasetInfos 

            #TODO export zipped and index files and link on other side.

        def identify_dataset_files(self):
            """This must return as list formatted as BaseFileHandler.identify_dataset_files() states."""

            # Note that index_and_zip is updating self._filecollector._datasetInfos[0]['path']
            return self.index_and_zip()       
    

    # Makes a FileHandler class composed of the validator.
    validator_class = FileValidator()
    worker = VCFFileHandler(validator_class)



    print >> sys.stderr, '-----NAMES:', worker._exporter._dataset_files_for_export

    # Updates for value for method in exporter. 
    # need to update this.
    worker._exporter._dataset_files_for_export = worker._filecollector._datasetInfos


    print >> sys.stderr, '-----NAMES:', worker._filecollector._datasetInfos
    print >> sys.stderr, '-----NAMES:', worker._exporter._dataset_files_for_export
    files = worker.identify_dataset_files()
    print >> sys.stderr, '-----NAMES a:', files

    # Runs the export. 
    worker._exporter.export(files)
    # worker.test()

if __name__ == "__main__":
    sys.exit(main())
